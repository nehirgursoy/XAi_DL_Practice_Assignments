{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Derivatives and Graphs \\[25 points\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<b><span style=\"color:#00CED1\">Association:</span></b>\n",
    "<span style=\"color:#00CED1\">Otto-Friedrich University of Bamberg</span>\n",
    "<span style=\"color:#00CED1\">Chair of Explainable Machine Learning (xAI)</span>\n",
    "<span style=\"color:#00CED1\">Deep Learning Assignments</span>\n",
    "\n",
    "<b><span style=\"color:#00CED1\">Description:</span></b>\n",
    "<span style=\"color:#00CED1\">This notebook introduces how to compute derivatives and gradients for PyTorch tensors.</span>\n",
    "<span style=\"color:#00CED1\">Students will learn these prinicples and are able to test their implementations directly via provided unittests.</span>\n",
    "\n",
    "<span style=\"color:#00CED1\"><b>Author:</b> Sebastian Doerrich</span>\n",
    "<span style=\"color:#00CED1\"><b>Copyright:</b> Copyright (c) 2022, Chair of Explainable Machine Learning (xAI), Otto-Friedrich University of Bamberg</span>\n",
    "<span style=\"color:#00CED1\"><b>Credits:</b> Christian Ledig, Sebastian Doerrich</span>\n",
    "<span style=\"color:#00CED1\"><b>License:</b> CC BY-SA</span>\n",
    "<span style=\"color:#00CED1\"><b>Version:</b> 1.0</span>\n",
    "<span style=\"color:#00CED1\"><b>Python:</b> Python 3</span>\n",
    "<span style=\"color:#00CED1\"><b>Maintainer:</b> Sebastian Doerrich</span>\n",
    "<span style=\"color:#00CED1\"><b>Email:</b> sebastian.doerrich@uni-bamberg.de</span>\n",
    "<span style=\"color:#00CED1\"><b>Status:</b> Production</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Context\n",
    "Welcome one last time to your first assignment.\n",
    "This fourth and last part of the assignment gives you an introduction on how to compute derivatives and gradients for Pytorch tensors. When applying deep neural networks, one always has to compute gradients of multiple different network parameters. Hence, understanding the fundamentals now will help you succeed in later assignments.\n",
    "\n",
    "## Instructions\n",
    "- You will be using Python 3.\n",
    "- After coding your function, run the cell right below it to check if your result is correct.\n",
    "\n",
    "## Important Notes for Your Submission\n",
    "Before submitting your assignment, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "If you do any of the mentioned, our test scripts will fail and as a result you will receive **0 points** for the respective task.\n",
    "\n",
    "## Table of Contents\n",
    "- [0 - Import the Necessary Libraries](#0)\n",
    "- [1 - Plotting](#1)\n",
    "- [2 - Derivatives \\[17.5 points\\]](#2)\n",
    "    - [2.1 - Derivative of a Simple Function at a Specific Position \\[5 points\\]](#2-1)\n",
    "    - [2.2 - Derivative of a More Complicated Function at a Specific Position \\[5 points\\]](#2-2)\n",
    "    - [2.3 - Derivative of an Entire Function \\[7.5 points\\]](#2-3)\n",
    "- [3 - Partial Derivatives \\[7.5 points\\]](#3)\n",
    "- [4 - End of Exercise](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='0'></a>\n",
    "## 0 - Import the Necessary Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu118'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Test for PyTorch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Plotting ##\n",
    "\n",
    "This function will allow you to visualize the functions and their derivatives of later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot(x: torch.tensor, f_x: torch.tensor, marker=False):\n",
    "    \"\"\"\n",
    "    Plot a function together with its derivative.\n",
    "\n",
    "    :param x: Values on the x-axis.\n",
    "    :param f_x: Function values.\n",
    "    \"\"\"\n",
    "\n",
    "    if marker:\n",
    "        fig, (ax1, ax2)= plt.subplots(figsize=(8, 6), dpi=80, nrows=1, ncols=2)\n",
    "\n",
    "        ax1.plot(x.detach().numpy(), f_x.detach().numpy(), color='red', marker='o', markersize=10, label = 'function')\n",
    "        ax2.plot(x.detach().numpy(), x.grad.detach().numpy(), color='blue', marker='o', markersize=10, label = 'derivative')\n",
    "\n",
    "        ax1.set_xlabel('x')\n",
    "        ax2.set_xlabel('x')\n",
    "        ax1.legend()\n",
    "        ax2.legend()\n",
    "\n",
    "        fig.suptitle('Function $f(x)=x^2$ together with its derivative $f\\'(x)$ at position $x=2$')\n",
    "\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6), dpi=80)\n",
    "\n",
    "        ax.plot(x.detach().numpy(), f_x.detach().numpy(), label = 'function')\n",
    "        ax.plot(x.detach().numpy(), x.grad.detach().numpy(), label = 'derivative')\n",
    "\n",
    "        ax.set_xlabel('x')\n",
    "        ax.legend()\n",
    "\n",
    "        fig.suptitle('Function $f(x)$ together with its derivative $f\\'(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Derivatives \\[17.5 points\\] ##\n",
    "\n",
    "In this exercise you will learn how to calculate the gradient (derivative) of basic functions which do calculations based on Pytorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 - Derivative of a Simple Function at a Specific Position \\[5 points\\] ###\n",
    "\n",
    "In this exercise you will learn how to calculate the derivative of the simple function $f(x)$ given as:\n",
    "\\begin{equation}\n",
    "    f(x) = x^2\n",
    "\\end{equation}\n",
    "at position $x=2$.\n",
    "\n",
    "\n",
    "Remember the derivative $f'(x)$ of the function $f(x)$ with respect to $x$ is calculated as:\n",
    "\\begin{equation}\n",
    "    f'(x) = \\frac{df(x)}{dx} = (x^2)' = 2x\n",
    "\\end{equation}\n",
    "which means that $f'(x)$ for $x=2$ is:\n",
    "\\begin{equation}\n",
    "    f'(x=2) = 4\n",
    "\\end{equation}\n",
    "\n",
    "Of course, we could hard code all of that and compute our derivative. However, we would do it only for a single function at a single evaluation position. This means that we would need to code everything again for a different function or a different position we want our derivative to be evaluated at. Hence, this is not quite efficient.\n",
    "\n",
    " But since each one of you is a highly skilled programmer and probably wants to stay one, you want to use a more convenient and efficient way to compute derivatives of different functions. And for that Pytorch will give you the answer.\n",
    " In the next few steps, we will learn how to use a few simple commands provided by Pytorch to achieve that goal.\n",
    "\n",
    "<b><span style=\"color:teal\">TODO:</span> <b>\n",
    "<dl>\n",
    "<dd><span style=\"color:teal\">Implement the method <span style=\"color:#DC143C\"><em>derivative_of_simple_function_at_position</em></span> which calculates the derivative of $f(x)$ at $x=2$.</span></dd>\n",
    "</dl>\n",
    "\n",
    "<b><span style=\"color:#B8860B\">Hints:</span> <b>\n",
    "<dl>\n",
    "<dd><span style=\"color:#B8860B\">1. Check out the <em><a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html\"><u>requires_grad</u></a></em> option for Pytorch tensors.</span></dd>\n",
    "<dd><span style=\"color:#B8860B\">2. Check out the function <em><a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\"><u>backward()</u></a></em> provided by Pytorch for tensors.</span></dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def derivative_of_simple_function_at_position():\n",
    "    \"\"\"\n",
    "    Calculate the derivative f'(x) = (x^2)' with respect to x at position x=2.\n",
    "\n",
    "    :return: The derivative of f(x) = x^2 with respect to x at position x=2 as well as x.\n",
    "    \"\"\"\n",
    "\n",
    "    x = None\n",
    "    f_x = None\n",
    "\n",
    "    #############################################################################\n",
    "    #                            START OF YOUR CODE                             #\n",
    "    # TODO:                                                                     #\n",
    "    #    1) Create x as a tensor with a single element which has the value 2.0  #\n",
    "    #       which allows for the Pytorch gradient computation (see Hints)       #\n",
    "    #    2) Calculate f(x)                                                      #\n",
    "    #    3) Calculate f'(x) with respect to x (see Hints)                       #\n",
    "    #############################################################################\n",
    "    x = torch.tensor(2.0, requires_grad = True)\n",
    "    f_x = x**2\n",
    "    f_x.backward()\n",
    "\n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "\n",
    "    return x, f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot your solution\n",
    "x, f_x = derivative_of_simple_function_at_position()\n",
    "\n",
    "plot(x, f_x, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t3\"]\n",
      ".C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t2\"]\n",
      ".C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t1\"]\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Test your code\n",
    "\n",
    "!python ./tests/test_derivatives_and_graphs.py --test_case TestDerivativeSimpleFunctionAtPosition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Derivative of a More Complicated Function at a Specific Position \\[5 points\\] ###\n",
    "\n",
    "Let's try what we have learned above for a more complicated function $g(x)$.\n",
    "The function $g(x)$ is given as:\n",
    "\\begin{equation}\n",
    "    g(x) = 2x^3 + x^2 + x\n",
    "\\end{equation}\n",
    "And we want to calculate its derivative $g'(x)$ at position $x=5$.\n",
    "\n",
    "<b><span style=\"color:teal\">TODO:</span> <b>\n",
    "<dl>\n",
    "<dd><span style=\"color:teal\">Implement the method <span style=\"color:#DC143C\"><em>derivative_of_more_complicated_function_at_position</em></span> which calculates the derivative of $g(x)$ at $x=5$.</span></dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def derivative_of_more_complicated_function_at_position():\n",
    "    \"\"\"\n",
    "    Calculate the derivative g'(x) = (2x^3 + x^2 + x)' with respect to x at position x=5.\n",
    "\n",
    "    :return: The function g(x) = 2x^3 + x^2 + x at x=5 as well as x.\n",
    "    \"\"\"\n",
    "\n",
    "    x = None\n",
    "    g_x = None\n",
    "\n",
    "    #############################################################################\n",
    "    #                            START OF YOUR CODE                             #\n",
    "    # TODO:                                                                     #\n",
    "    #    1) Create x as a tensor with a single element which has the value 5.0  #\n",
    "    #       which allows for the Pytorch gradient computation                   #\n",
    "    #    2) Calculate g(x)                                                      #\n",
    "    #    3) Calculate g'(x) with respect to x                                   #\n",
    "    #############################################################################\n",
    "    x = torch.tensor(5.0, requires_grad=True)\n",
    "    g_x = 2* x**3 + x**2 + x\n",
    "    g_x.backward()\n",
    "\n",
    "\n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "\n",
    "    return x, g_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot your solution\n",
    "x, g_x = derivative_of_more_complicated_function_at_position()\n",
    "\n",
    "plot(x, g_x, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:109: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t5\"]\n",
      ".C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t6\"]\n",
      ".C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t4\"]\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Test your code\n",
    "\n",
    "!python ./tests/test_derivatives_and_graphs.py --test_case TestDerivativeComplicatedFunctionAtPosition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='2-3'></a>\n",
    "### 2.3 - Derivative of an Entire Function \\[7.5 points\\] ###\n",
    "\n",
    "Having the derivative at a specific position $x^\\star$ is cool, but in general we are interested in the derivative of an entire function.\n",
    "Calculation of the latter can also be done quite fast by using the autograd functionality of Pytorch.\n",
    "\n",
    "Let's take a look at the Rectified Linear Unit (ReLU) function which is given as:\n",
    "\\begin{equation}\n",
    "    f(x) = max(0, x)\n",
    "\\end{equation}\n",
    "\n",
    "We want to calculate the derivative of the entire function at every point of $x$.\n",
    "\n",
    "<b><span style=\"color:teal\">TODO:</span> <b>\n",
    "<dl>\n",
    "<dd><span style=\"color:teal\">Implement the method <span style=\"color:#DC143C\"><em>derivative_of_relu</em></span> which calculates the derivative $f'(x)$ of $f(x)$ at every point in $x$.</span></dd>\n",
    "</dl>\n",
    "\n",
    "<b><span style=\"color:#B8860B\">Hints:</span> <b>\n",
    "<dl>\n",
    "<dd><span style=\"color:#B8860B\">1. Check out the <em><a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2022-01-01\"><u>sum()</u></a></em>-trick provided by Pytorch to get the derivative of a vector valued function.</span></dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def derivative_of_relu():\n",
    "    \"\"\"\n",
    "    Calculate the derivative of the relu function.\n",
    "\n",
    "    :return: The relu function as well as x.\n",
    "    \"\"\"\n",
    "\n",
    "    x = None\n",
    "    f_x = None\n",
    "\n",
    "    #############################################################################\n",
    "    #                            START OF YOUR CODE                             #\n",
    "    # TODO:                                                                     #\n",
    "    #    1) Create x as a tensor with 1000 values in between [-10, 10] which    #\n",
    "    #       allows for the Pytorch gradient computation                         #\n",
    "    #    2) Calculate the relu function f(x)                                    #\n",
    "    #    3) Apply the sum()-trick provided by Pytorch                           #\n",
    "    #    4) Calculate f'(x) with respect to x                                   #\n",
    "    #############################################################################\n",
    "    x = torch.linspace(-10, 10, 1000,requires_grad=True)\n",
    "    f_x = torch.relu(x)\n",
    "    f_x.sum().backward()\n",
    "    return x, f_x\n",
    "\n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "\n",
    "    return x, F_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot your solution\n",
    "x, f_x = derivative_of_relu()\n",
    "\n",
    "plot(x, f_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t9\"]\n",
      ".C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t8\"]\n",
      ".C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t7\"]\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Test your code\n",
    "\n",
    "!python ./tests/test_derivatives_and_graphs.py --test_case TestDerivativeEntireFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Partial Derivatives \\[7.5 points\\] ##\n",
    "\n",
    "In this exercise you will learn how to calculate partial derivatives of functions which do calculations based on Pytorch tensors.\n",
    "\n",
    "Consider the function $f(x)$ given as:\n",
    "\\begin{equation}\n",
    "    f(u, v) = v*u + u^2 + 3*v\n",
    "\\end{equation}\n",
    "We want to calculate the derivatives $f_{u}$ and $f_{v}$ of $f(u, v)$ with respect to $u$ and $v$ respectively.\n",
    "\n",
    "<b><span style=\"color:teal\">TODO:</span> <b>\n",
    "<dl>\n",
    "<dd><span style=\"color:teal\">Implement the method <span style=\"color:#DC143C\"><em>partial_derivatives</em></span> which calculates the derivatives $f_{u}$ and $f_{v}$ of $f(u, v)$ with respect to $u$ and $v$ respectively.</span></dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def partial_derivatives():\n",
    "    \"\"\"\n",
    "    Calculate the partial derivatives f_u and f_v the function f(u, v) = v*u + u^2 + 3*v.\n",
    "\n",
    "    :return: The function f(u, v) as well as u and v.\n",
    "    \"\"\"\n",
    "\n",
    "    u = None\n",
    "    v = None\n",
    "    f_uv = None\n",
    "\n",
    "    #############################################################################\n",
    "    #                            START OF YOUR CODE                             #\n",
    "    # TODO:                                                                     #\n",
    "    #    1) Create u as a tensor with 100 values in between [-10, 10] which     #\n",
    "    #       allows for the Pytorch gradient computation                         #\n",
    "    #    2) Create v as a tensor with 100 values in between [-10, 10] which     #\n",
    "    #       allows for the Pytorch gradient computation                         #\n",
    "    #    3) Calculate the function f(u, v)                                      #\n",
    "    #    4) Apply the sum()-trick provided by Pytorch                           #\n",
    "    #    5) Calculate f_u and f_v with respect to u and v respectively          #\n",
    "    #############################################################################\n",
    "    u = torch.linspace(-10, 10,100, requires_grad = True)\n",
    "    v = torch.linspace(-10, 10, 100, requires_grad = True)\n",
    "\n",
    "    f_uv = v*u + u**2 + 3*v\n",
    "    f_uv_sum =f_uv.sum()\n",
    "    f_uv_sum.backward()\n",
    "    f_uv = F_uv\n",
    "\n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "\n",
    "    return u, v, F_uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:226: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t14\"]\n",
      ".C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:205: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t12\"]\n",
      ".C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:216: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t13\"]\n",
      ".C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t10\"]\n",
      ".C:\\Users\\Dell\\OneDrive\\Desktop\\xAI-DL\\assignment1\\assignment1\\Programming\\code\\tests\\test_derivatives_and_graphs.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_version = torch.load(\"../data/derivatives_and_graphs/references.pt\")[\"t11\"]\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.006s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Test your code\n",
    "\n",
    "!python ./tests/test_derivatives_and_graphs.py --test_case TestPartialDerivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='4'></a>\n",
    "## 4 - End of Exercise ##\n",
    "\n",
    "<div>\n",
    "    <img src=\"../img/memes/meme_youDidIt_04.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "Created with and licensed under [Adobe Express](https://www.adobe.com/de/express/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
